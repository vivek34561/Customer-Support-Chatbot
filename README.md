# ğŸ¤– Intelligent Customer Support Chatbot â€“ Cost-Optimized RAG System

<p align="center">
  <img src="https://img.shields.io/badge/python-3.10+-blue.svg" alt="Python"/>
  <img src="https://img.shields.io/badge/LangGraph-latest-green.svg" alt="LangGraph"/>
  <img src="https://img.shields.io/badge/FastAPI-latest-009688.svg" alt="FastAPI"/>
  <img src="https://img.shields.io/badge/FAISS-Vector%20DB-orange.svg" alt="FAISS"/>
  <img src="https://img.shields.io/badge/Groq-LLM-purple.svg" alt="Groq"/>
  <img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="License"/>
</p>

<p align="center">
  <strong>A production-grade customer support chatbot with intelligent 3-tier routing achieving 79.6% cost reduction through adaptive RAG and sentiment-based escalation.</strong>
</p>

<p align="center">
  <a href="#-features">âœ¨ Features</a> â€¢
  <a href="#-architecture">ğŸ—ï¸ Architecture</a> â€¢
  <a href="#-quick-start">ğŸš€ Quick Start</a> â€¢
  <a href="#-performance-metrics">ğŸ“Š Performance</a> â€¢
  <a href="#-project-structure">ğŸ“‚ Project Structure</a> â€¢
  <a href="#-deployment">ğŸš€ Deployment</a>
</p>

---

## ğŸ¯ Overview

This is a **production-ready customer support chatbot** built with **LangChain, LangGraph, and FastAPI** that intelligently routes queries through a cost-optimized 3-tier system.

Instead of processing all queries equally, the system:

* Routes **FAQ queries** to template responses (zero cost)
* Uses **RAG + Local LLM** for procedural questions (low cost)
* Escalates **complex/sensitive issues** to human agents (high cost)
* Applies **sentiment analysis** to detect frustrated customers and prioritize them
* Achieves **97.69% intent classification accuracy** on 27 intents

This results in **79.6% cost reduction** while maintaining high-quality responses.

---

## ğŸ’¡ Why This Project?

**The Problem:**
Traditional chatbots either:
* Use expensive LLMs for every query (high cost)
* Use templates for everything (poor user experience)
* Don't detect customer emotion (frustrated customers get automated responses)

**The Solution:**
Build a **self-routing, emotion-aware chatbot** that:
* Decides *how* to answer based on query complexity
* Uses *sentiment analysis* to catch angry customers
* Optimizes *cost* without sacrificing quality
* Provides a *production REST API* for integration

**Key Innovation:**
Combines intent classification, sentiment analysis, and adaptive RAG in a unified LangGraph workflow.

---

## âœ¨ Key Features

### ğŸ¯ Intelligent Routing
* **3-Bucket System**: Zero-cost templates â†’ Low-cost RAG â†’ High-cost escalation
* **97.69% Accuracy**: TF-IDF + Logistic Regression intent classifier (27 intents)
* **Confidence Fallback**: Low-confidence queries safely routed to RAG

### ğŸ˜Š Sentiment-Based Escalation
* **Hybrid Analysis**: Combines DistilBERT sentiment + keyword detection
* **Smart Override**: Routes angry customers to human support even for simple queries
* **False Positive Prevention**: Keyword filter prevents neutral questions from escalating

### ğŸ§  Local-First Architecture
* **HuggingFace Embeddings**: sentence-transformers/all-MiniLM-L6-v2 (runs offline)
* **FAISS Vector Database**: No cloud dependencies, version-controllable index
* **Groq LLM**: Ultra-fast inference (750+ tokens/sec), free tier available

### ğŸš€ Production Ready
* **FastAPI REST API**: 5 endpoints with auto-generated docs
* **Web Chat UI**: Beautiful responsive interface
* **Lazy Loading**: Models load on first request (deployment-friendly)
* **Response Cleaning**: Automatically removes LLM thinking tags and formatting issues

---

## ğŸ“Š Performance Metrics

| Metric | Value |
|--------|-------|
| **Cost Reduction** | 79.6% vs uniform LLM |
| **Intent Accuracy** | 97.69% (27 intents) |
| **Zero-Cost Routing** | 30.6% of queries |
| **Low-Cost Routing** | 51.6% of queries |
| **High-Cost Routing** | 17.8% of queries |
| **Dataset Size** | 26,872 examples |
| **Response Time** | <2s (after model load) |

---

## ğŸš€ Quick Start

### Prerequisites

* Python 3.10+
* Groq API key (free at [console.groq.com](https://console.groq.com))

### Environment Setup

Create a `.env` file:

```env
GROQ_API_KEY=your_groq_api_key_here
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
LLM_MODEL=llama-3.3-70b-versatile
ENABLE_SENTIMENT_ANALYSIS=true
```

### Install Dependencies

```bash
**Note:** Embedding model (~90MB) downloads automatically on first run and is cached locally.

### Run the Chatbot

#### Option 1: Streamlit UI (Recommended) ğŸ¨

**Quick Launch:**
```bash
# Windows
launch_streamlit.bat

# Linux/Mac
chmod +x launch_streamlit.sh
./launch_streamlit.sh
```

**Manual Launch:**
```bash
# Terminal 1: Start FastAPI backend
python api.py

# Terminal 2: Start Streamlit UI
streamlit run streamlit_app.py
```

The Streamlit UI will open automatically at `http://localhost:8501` with:
- ğŸ’¬ Real-time chat interface
- ğŸ“Š Live analytics dashboard
- ğŸ­ Sentiment tracking
- ğŸ¯ Intent visualization
- ğŸ“¦ Bucket distribution stats

#### Option 2: Interactive CLI

```bash
python src/main.py interactive
```

#### Option 3: REST API Only

```bash
# Start FastAPI server
python api.py

# API runs at http://localhost:8000
# Docs at http://localhost:8000/docs
```

#### Option 4: HTML Chat UI (Legacy)

Start the API then open `chat_ui.html` in your browser.

### Test the API

```bash
python test_api.py
```

---

## ğŸ“‚ Project Structure

```
Customer Support Chatbot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                      # Configuration & environment
â”‚   â”œâ”€â”€ faiss_index_builder.py         # FAISS index builder
â”‚   â”œâ”€â”€ retriever.py                   # RAG retriever
â”‚   â”‚
â”‚   â”œâ”€â”€ state/
â”‚   â”‚   â””â”€â”€ state.py                   # ChatbotState TypedDict
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ models.py                  # Groq LLM setup
â”‚   â”‚   â””â”€â”€ prompts.py                 # System prompts & templates
â”‚   â”‚
â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”œâ”€â”€ intent_node.py             # Intent + sentiment analysis
â”‚   â”‚   â”œâ”€â”€ retrieve_node.py           # FAISS retrieval
â”‚   â”‚   â””â”€â”€ generate_node.py           # LLM generation + cleaning
â”‚   â”‚
â”‚   â”œâ”€â”€ graph/
â”‚   â”‚   â””â”€â”€ chatbot_graph.py           # LangGraph workflow
â”‚   â”‚
â”‚   â””â”€â”€ main.py                        # Main interface
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl           # TF-IDF model
â”‚   â”œâ”€â”€ intent_classifier.pkl          # Logistic Regression
â”‚   â””â”€â”€ routing_config.json            # Bucket mappings
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ faiss_index/                   # Vector DB (created at build)
â”‚
â”œâ”€â”€ Notebook/
â”‚   â””â”€â”€ sementic_analysis.ipynb        # Sentiment testing
â”‚
â”œâ”€â”€ api.py                             # FastAPI server
â”œâ”€â”€ streamlit_app.py                   # Streamlit UI (recommended)
â”œâ”€â”€ chat_ui.html                       # HTML UI (legacy)
â”œâ”€â”€ launch_streamlit.bat               # Windows launcher
â”œâ”€â”€ launch_streamlit.sh                # Linux/Mac launcher
â”œâ”€â”€ intent_router.py                   # Routing module
â”œâ”€â”€ build_rag_index.py                 # Index builder script
â”œâ”€â”€ test_api.py                        # API tests
â”œâ”€â”€ requirements.txt                   # Dependencies
â”œâ”€â”€ .env                               # API keys (create this)
â””â”€â”€ README.md                          # This file
```

---

## ğŸ§ª Testing & Evaluation

### Test Intent Classification

```bash
python intent_router.py
```

### Test Sentiment Analysis

```bash
python test_sentiment_routing.py
```

### Test Latency & Performance

```bash
python test_latency.py
```

**This will:**
* Test 11+ queries across all buckets (A, B, C)
* Measure client-side and server-side latency
* Report statistics (mean, median, min, max, stddev)
* Show bucket-wise performance breakdown
* Provide performance insights and recommendations

**Sample Output:**
```
ğŸ“Š LATENCY STATISTICS
Overall Performance:
  Total Requests: 11
  Successful: 11

Server Processing Time:
  Mean:   145.32 ms
  Median: 89.45 ms
  Min:    45.12 ms
  Max:    542.78 ms

Bucket Performance Breakdown:
BUCKET_A (4 requests):
  Mean:   52.34 ms   (fastest - template responses)
  
BUCKET_B (5 requests):
  Mean:   198.67 ms  (RAG + LLM generation)
  
BUCKET_C (2 requests):
  Mean:   78.91 ms   (fast escalation)

ğŸ’¡ Performance Insights:
  â€¢ BUCKET_A is 3.8x faster than BUCKET_B
  â€¢ Average network overhead: 12.3 ms
```

### Dry-Run Evaluation (500 samples)

```bash
python dry_run_evaluation.py
```

### Test RAG Retrieval

```bash
python src/retriever.py
```

---

## ğŸ”¬ How It Works

### 1. Intent Classification

* **Model**: TF-IDF + Logistic Regression
* **Accuracy**: 97.69% on 27 intents
* **Processing**: <10ms per query
* **Output**: Intent label + confidence score

**Example:**
```
Input: "How do I track my order?"
Output: track_order (98% confidence) â†’ BUCKET_A
```

### 2. Sentiment Analysis (Hybrid Approach)

* **Model**: DistilBERT SST-2 (sentiment) + keyword filter (anger detection)
* **Logic**: 
  - Classify sentiment (POSITIVE/NEGATIVE)
  - Check for anger keywords (terrible, frustrated, useless, etc.)
  - Override bucket to BUCKET_C if: NEGATIVE + high confidence + anger detected

**Example:**
```
Input: "This is terrible! I want my money back!"
Sentiment: NEGATIVE (92%) + anger keywords detected
Action: Override BUCKET_A â†’ BUCKET_C (escalate)
```

### 3. FAISS Retrieval (BUCKET_B only)

* **Embeddings**: sentence-transformers/all-MiniLM-L6-v2 (384 dim)
* **Index**: IndexFlatIP (cosine similarity)
* **Top-K**: 3 most relevant documents
* **Storage**: Local files (no cloud)

### 4. Response Generation

* **BUCKET_A**: Return template (no LLM)
* **BUCKET_B**: RAG prompt + Groq LLM + response cleaning
* **BUCKET_C**: Escalation message

**Response Cleaning:**
- Removes `<think>...</think>` tags
- Removes internal reasoning
- Cleans whitespace

---

## ğŸ¯ Three-Bucket Routing System

### BUCKET_A: Zero-Cost (30.6% of queries)

**Intents:** 
- check_invoice
- check_payment_methods
- check_refund_policy
- check_cancellation_fee
- delivery_period
- delivery_options
- track_order
- track_refund

**Handling:** Template responses, no LLM
**Cost:** $0

### BUCKET_B: Low-Cost (51.6% of queries)

**Intents:**
- cancel_order, change_order
- create_account, edit_account, delete_account
- get_invoice, get_refund
- change_shipping_address, set_up_shipping_address
- place_order, recover_password
- registration_problems, newsletter_subscription
- review, switch_account

**Handling:** FAISS retrieval â†’ Groq generation
**Cost:** ~$0.0001 per query

### BUCKET_C: High-Cost (17.8% of queries)

**Intents:**
- complaint
- payment_issue
- contact_customer_service
- contact_human_agent

**Handling:** Escalation message or human handoff
**Cost:** Variable
**Trigger:** Intent-based OR sentiment override

---

## ğŸš€ Deployment

### Local Development

```bash
python api.py
# Access at http://localhost:8000
```

### Production Deployment

**Supported Platforms:**
- Railway.app (Recommended - $5 free credit/month)
- Fly.io (Free tier: 256MB x 3 machines)
- Render.com (Free tier: 512MB RAM - requires optimization)

**Deployment Guide:** See [DEPLOYMENT.md](DEPLOYMENT.md)

**Memory Optimization:**
- Lazy loading enabled by default
- Sentiment analysis optional (`ENABLE_SENTIMENT_ANALYSIS=false` saves ~200MB)
- Recommended: 2GB RAM for full features

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | API information |
| `/health` | GET | Health check |
| `/chat` | POST | Process message |
| `/intents` | GET | List all intents |
| `/stats` | GET | Performance metrics |

**API Documentation:** `http://localhost:8000/docs` (auto-generated)

#### Chat Endpoint Example

```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "How do I track my order?"}'
```

**Response includes latency measurement:**
```json
{
  "response": "To track your order, please visit...",
  "intent": "track_order",
  "confidence": 0.98,
  "bucket": "BUCKET_A",
  "cost_tier": "Zero",
  "action": "Direct template response",
  "sentiment": "POSITIVE",
  "sentiment_score": 0.89,
  "escalated_by_sentiment": false,
  "latency_ms": 52.34,
  "session_id": null
}
```

**Latency Monitoring:**
* `latency_ms`: Server-side processing time (in milliseconds)
* Response headers include `X-Process-Time` for total request time
* Console logs show timing for every request: `â±ï¸ POST /chat - 0.052s`

---

## ğŸ“Š What Makes This Project Strong

âœ… **Production-Grade Architecture**
- Not a demo - ready for real deployment
- FastAPI with proper error handling
- Lazy loading for deployment optimization

âœ… **Cost Optimization**
- 79.6% cost reduction proven on 500-query evaluation
- Smart routing means most queries use zero/low-cost paths

âœ… **Emotion Intelligence**
- Sentiment analysis catches frustrated customers
- Hybrid approach prevents false positives

âœ… **Real ML Engineering**
- 97.69% classification accuracy
- Proper train/test split
- Confidence-based fallbacks

âœ… **Modern Stack**
- LangGraph state machines
- Local-first (works offline after setup)
- Clean separation of concerns

This is the kind of chatbot used in:
* E-commerce customer support
* SaaS helpdesks
* Internal IT support
* Banking/fintech support

---

## ğŸ”® Future Improvements

- [ ] Multi-language support
- [ ] Conversation memory (chat history)
- [ ] Streaming responses
- [ ] Custom knowledge base upload
- [ ] Fine-tuned intent classifier
- [ ] A/B testing framework
- [ ] Analytics dashboard
- [ ] Docker containerization

---

## ğŸ‘¨â€ğŸ’» Author

**Vivek Kumar Gupta**
AI Engineering Student | GenAI & Agentic Systems Builder

* **GitHub**: [https://github.com/vivek34561](https://github.com/vivek34561)
* **LinkedIn**: [https://linkedin.com/in/vivek-gupta-0400452b6](https://linkedin.com/in/vivek-gupta-0400452b6)
* **Portfolio**: [https://resume-sepia-seven.vercel.app/](https://resume-sepia-seven.vercel.app/)

---

## ğŸ™ Acknowledgments

- **Bitext** for the customer support dataset
- **HuggingFace** for sentence-transformers and DistilBERT
- **Meta AI** for FAISS vector database
- **Groq** for lightning-fast LLM inference
- **LangChain team** for LangGraph framework

---

## ğŸ“„ License

MIT License Â© 2025 Vivek Kumar Gupta

---

## ğŸš€ Ready to Run?

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Set up .env with GROQ_API_KEY

# 3. Build FAISS index
python build_rag_index.py --limit 100

# 4. Run interactively
python src/main.py interactive

# OR run API
python api.py
### FAISS Vector Database
- âœ… No cloud service needed
- âœ… No API keys for vector DB
- âœ… Fast for small-medium datasets
- âœ… Can version control the index

### Groq LLM
- âœ… Ultra-fast inference (750+ tokens/sec)
- âœ… Free tier available
- âœ… Low latency
- âœ… Cost-effective

## ğŸ“¦ Dependencies

```
# Core
pandas, numpy, scikit-learn

# Embeddings
sentence-transformers, torch

# Vector DB
faiss-cpu

# LLM
groq

# Orchestration
langchain, langchain-groq, langgraph
```

## ğŸ”‘ API Keys Needed

**Required:**
- Groq API Key (free at console.groq.com)

**Optional:**
- None! Embeddings and vector DB are local

## ğŸ“ˆ Cost Comparison

**Traditional Approach (uniform GPT-4):**
- 100% of queries â†’ GPT-4 â†’ High cost

**Our Approach:**
- 30.6% â†’ Zero cost (templates)
- 51.6% â†’ Low cost (Groq)
- 17.8% â†’ High cost (escalation)
- **Result: 79.6% cost reduction**

## ğŸ“ Dataset

Bitext Customer Support Dataset v11
- **Size:** 26,872 instruction-response pairs
- **Intents:** 27 categories
- **Language:** English
- **Domain:** E-commerce customer support

## ğŸ¤ Contributing

Feel free to open issues or submit PRs!

## ğŸ“„ License

See LICENSE file for details.

## ğŸ™ Acknowledgments

- Bitext for the customer support dataset
- HuggingFace for sentence-transformers
- Meta for FAISS
- Groq for fast LLM inference
- LangChain team for LangGraph

---

**Ready to run?**
```bash
pip install -r requirements-rag.txt
python build_rag_index.py --limit 100
python src/main.py interactive
```
